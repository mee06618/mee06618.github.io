---
title: "html 크롤링"
date: 2021-08-19 16:41:28 -0400
categories: study
---
- html 크롤링
    ```
from urllib.request import urlopen

from bs4 import BeautifulSoup

import re

import datetime

import random

 

pages = set()

random.seed(datetime.datetime.now())

 

# 페이지에서 발견된 내부 링크를 모두 목록으로 만듭니다.

def getInternalLinks(bsObj, includeUrl):

  internalLinks = []

  # /로 시작하는 링크를 모두 찾습니다.

  for link in bsObj.findAll("a", href=re.compile("^(/|.*"+includeUrl+")")):

    if link.attrs['href'] is not None:

      if link.attrs['href'] not in internalLinks:

        internalLinks.append(link.attrs['href'])

    return internalLinks

 

# 페이지에서 발견된 외부 링크를 모두 목록으로 만듭니다.

def getExternalLinks(bsObj, excludeUrl):

  externalLinks = []

  # 현재 URL을 포함하지 않으면서 http나 www로 시작하는 링크를 모두 찾습니다.

  for link in bsObj.findAll("a",

                            href=re.compile("^(http|www)((?!"+excludeUrl+").)*$")):

      if link.attrs['href'] is not None:

          if link.attrs['href'] not in externalLinks:

            externalLinks.append(link.attrs['href'])

   return externalLinks

 

def splitAddress(address):

    addressParts = address.replace("http://", "").split("/")

    return addressParts

 

def getRandomExternalLink(startingPage):

  html = urlopen(startingPage)

  bsObj = BeautifulSoup(html, "html.parser")

  externalLinks = getExternalLinks(bsObj, splitAddress(startingPage)[0])

  if len(externalLinks) == 0:

    internalLinks = getInternalLinks(startingPage)

    return getNextExternalLink(internalLinks[random.randint(0,

                               len(internalLinks)-1)])

 

else:

   return externalLinks[random.randint(0, len(externalLinks)-1)]

 

def followExternalOnly(startingSite):

  externalLink = getRandomExternalLink("http://oreilly.com")

  print("Random external link is: "+externalLink)

  followExternalOnly(externalLink)

 

followExternalOnly("http://oreilly.com")
    ```
## 자주 하는 실수
  - 기본기능으로 때우려고함
## 큐
  - 어차피 헷갈리니 자주보고쓰자
## 내가 모르는 것
  - html 크롤링
      
